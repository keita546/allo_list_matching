# -*- coding: utf-8 -*-
"""
ç´¯ç©ãƒªã‚¹ãƒˆçµ±åˆãƒ—ãƒ­ã‚°ãƒ©ãƒ ï¼ˆé€±æ¬¡æ›´æ–°å°‚ç”¨ãƒ»å®Œå…¨ç‰ˆï¼‰

ã€å‡¦ç†ãƒ•ãƒ­ãƒ¼ã€‘
Week1: èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆï¼ˆä»»æ„ï¼‰ + ãƒãƒƒãƒãƒ³ã‚° â†’ ç´¯ç©ãƒªã‚¹ãƒˆ
Week2ä»¥é™: å‰é€±ã®ç´¯ç© + ä»Šé€±ã®ãƒãƒƒãƒãƒ³ã‚° â†’ æ›´æ–°ã•ã‚ŒãŸç´¯ç©ãƒªã‚¹ãƒˆ

ã€å„ªå…ˆé †ä½ã€‘
1. å‰é€±ã®ç´¯ç©ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰
2. ä»Šé€±ã®èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆï¼ˆåŠå¹´ã«1å›ã®ã¿ï¼‰
3. ä»Šé€±ã®ãƒãƒƒãƒãƒ³ã‚°çµæœ

Author: HIBI KEITA
Version: 2.0
"""

import pandas as pd
from pathlib import Path
import tkinter as tk
from tkinter import filedialog, messagebox
from datetime import datetime


# ========================================================================
# ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿é–¢æ•°
# ========================================================================

def load_file_flexible(file_path: str) -> pd.DataFrame:
    """
    CSV/TSV/Excelã‚’è‡ªå‹•åˆ¤åˆ¥ã—ã¦èª­ã¿è¾¼ã¿
    """
    p = Path(file_path)
    ext = p.suffix.lower()
    
    print(f"ğŸ“– ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­: {p.name}")
    
    # Excel
    if ext in ['.xlsx', '.xls', '.xlsm']:
        try:
            df = pd.read_excel(file_path, engine='openpyxl', dtype=str)
            print(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ")
            return df
        except Exception as e:
            raise ValueError(f"Excelèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    # CSV/TSV
    encodings = ['utf-8', 'shift_jis', 'cp932']
    delimiter_map = {
        '.csv': ',',
        '.tsv': '\t',
        '.txt': '\t',
    }
    
    if ext not in delimiter_map:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆå¤–ã®æ‹¡å¼µå­: {ext}")
    
    delimiter = delimiter_map[ext]
    
    for encoding in encodings:
        try:
            df = pd.read_csv(file_path, encoding=encoding, delimiter=delimiter, 
                           on_bad_lines='skip', dtype=str)
            print(f"âœ… {ext}ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸï¼ˆ{encoding}ï¼‰")
            return df
        except UnicodeDecodeError:
            continue
        except Exception as e:
            raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    
    raise UnicodeDecodeError(f"ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿å¤±æ•—")


# ========================================================================
# ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–é–¢æ•°
# ========================================================================

def normalize_columns(df: pd.DataFrame, source: str) -> pd.DataFrame:
    """
    ã‚«ãƒ©ãƒ åã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›
    
    çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
    - æ—§JANã‚³ãƒ¼ãƒ‰
    - æ—§å•†å“å
    - æ–°JANã‚³ãƒ¼ãƒ‰
    - æ–°å•†å“å
    - ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°
    - å‚™è€ƒ
    - å‡¦ç†æ—¥
    - ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹
    """
    print(f"ğŸ“ ã‚«ãƒ©ãƒ åæ­£è¦åŒ–ä¸­ï¼ˆ{source}ï¼‰...")
    
    # ã‚«ãƒ©ãƒ åã®ãƒãƒƒãƒ”ãƒ³ã‚°è¾æ›¸
    column_mapping = {
        # ãƒãƒƒãƒãƒ³ã‚°çµæœã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        'JANã‚³ãƒ¼ãƒ‰_æ—§': 'æ—§JANã‚³ãƒ¼ãƒ‰',
        'JANã‚³ãƒ¼ãƒ‰_æ–°': 'æ–°JANã‚³ãƒ¼ãƒ‰',
        'å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§': 'æ—§å•†å“å',
        'å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ–°': 'æ–°å•†å“å',
        'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ—§': 'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°',
        'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ–°': 'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°',
        
        # èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆã®ãƒ‘ã‚¿ãƒ¼ãƒ³
        'æ—§JAN': 'æ—§JANã‚³ãƒ¼ãƒ‰',
        'æ–°JAN': 'æ–°JANã‚³ãƒ¼ãƒ‰',
        
        # ãã®ä»–ã®ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒ³
        'JANæ—§': 'æ—§JANã‚³ãƒ¼ãƒ‰',
        'JANæ–°': 'æ–°JANã‚³ãƒ¼ãƒ‰',
    }
    
    # ã‚«ãƒ©ãƒ åã‚’å¤‰æ›
    df_normalized = df.rename(columns=column_mapping)
    
    # å¿…é ˆã‚«ãƒ©ãƒ ã®ç¢ºèªã¨è¿½åŠ 
    required_columns = {
        'æ—§JANã‚³ãƒ¼ãƒ‰': '',
        'æ—§å•†å“å': '',
        'æ–°JANã‚³ãƒ¼ãƒ‰': '',
        'æ–°å•†å“å': '',
        'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°': '',
        'å‚™è€ƒ': '',
        'å‡¦ç†æ—¥': datetime.now().strftime('%Y-%m-%d'),
        'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹': source,
    }
    
    for col, default_val in required_columns.items():
        if col not in df_normalized.columns:
            df_normalized[col] = default_val
    
    # çµ±ä¸€ã‚«ãƒ©ãƒ ã®ã¿æŠ½å‡º
    output_columns = list(required_columns.keys())
    df_normalized = df_normalized[output_columns].copy()
    
    print(f"âœ… æ­£è¦åŒ–å®Œäº†: {len(df_normalized)}ä»¶")
    return df_normalized


# ========================================================================
# JANã‚³ãƒ¼ãƒ‰ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
# ========================================================================

def clean_jan_codes(df: pd.DataFrame) -> pd.DataFrame:
    """
    JANã‚³ãƒ¼ãƒ‰ã‚’13æ¡ã«çµ±ä¸€
    """
    print("ğŸ§¹ JANã‚³ãƒ¼ãƒ‰ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°ä¸­...")
    
    for col in ['æ—§JANã‚³ãƒ¼ãƒ‰', 'æ–°JANã‚³ãƒ¼ãƒ‰']:
        if col in df.columns:
            df[col] = (df[col].astype(str)
                      .str.replace(r'\D+', '', regex=True)  # æ•°å­—ä»¥å¤–å‰Šé™¤
                      .str.zfill(13)  # 13æ¡ã«0åŸ‹ã‚
                      .str[:13])  # 13æ¡åˆ‡ã‚Šå–ã‚Š
    
    print("âœ… ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°å®Œäº†")
    return df


# ========================================================================
# é‡è¤‡å‰Šé™¤é–¢æ•°ï¼ˆå®Œå…¨ç‰ˆï¼‰
# ========================================================================

def remove_duplicates_advanced(existing_df: pd.DataFrame, 
                               kao_planet_df: pd.DataFrame, 
                               matching_df: pd.DataFrame) -> pd.DataFrame:
    """
    3æ®µéšã®å„ªå…ˆé †ä½ä»˜ãé‡è¤‡å‰Šé™¤
    
    ã€å„ªå…ˆé †ä½ã€‘
    1. æ—¢å­˜ã®ç´¯ç©ãƒªã‚¹ãƒˆï¼ˆå…¨ãƒ‡ãƒ¼ã‚¿ä¿æŒï¼‰
    2. èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå·®ã—æ›¿ãˆãƒªã‚¹ãƒˆï¼ˆåŠå¹´ã«1å›ï¼‰
    3. ãƒãƒƒãƒãƒ³ã‚°çµæœï¼ˆæ¯é€±ï¼‰
    
    ã€é‡è¤‡å‰Šé™¤ãƒ­ã‚¸ãƒƒã‚¯ã€‘
    - ã‚¹ãƒ†ãƒƒãƒ—1: ç´¯ç©å†…ã®èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆç”±æ¥ãƒ‡ãƒ¼ã‚¿ã®æ–°JANã‚’æŠ½å‡º
    - ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒãƒãƒ³ã‚°çµæœã‹ã‚‰ã€èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆç”±æ¥ã¨æ–°JANãŒè¢«ã‚‹ã‚‚ã®ã‚’å‰Šé™¤
    - ã‚¹ãƒ†ãƒƒãƒ—3: ä»Šé€±ã®èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆã¨ãƒãƒƒãƒãƒ³ã‚°ã§ã€æ—§JANãƒ»æ–°JANãŒè¢«ã‚‹ã‚‚ã®ã‚’å‰Šé™¤ï¼ˆãƒãƒƒãƒãƒ³ã‚°å´ã‚’å‰Šé™¤ï¼‰
    - ã‚¹ãƒ†ãƒƒãƒ—4: 3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰
    - ã‚¹ãƒ†ãƒƒãƒ—5: æ–°JANã§é‡è¤‡å‰Šé™¤ï¼ˆæœ€åˆã«å‡ºç¾ã—ãŸè¡Œã‚’æ®‹ã™ = å„ªå…ˆé †ä½ãŒé«˜ã„æ–¹ã‚’æ®‹ã™ï¼‰
    - ã‚¹ãƒ†ãƒƒãƒ—6: æ—§JAN=æ–°JANã®ã‚‚ã®ã‚’å‰Šé™¤
    """
    print("\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»é‡è¤‡å‰Šé™¤é–‹å§‹...")
    
    print(f"  ç´¯ç©ãƒ‡ãƒ¼ã‚¿: {len(existing_df)}ä»¶")
    print(f"  èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆï¼ˆä»Šé€±ï¼‰: {len(kao_planet_df)}ä»¶")
    print(f"  ãƒãƒƒãƒãƒ³ã‚°ï¼ˆä»Šé€±ï¼‰: {len(matching_df)}ä»¶")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—1: ç´¯ç©å†…ã®èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆç”±æ¥JANã‚’æŠ½å‡º ==========
    existing_kao_planet_jans = set()
    if not existing_df.empty:
        if 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹' in existing_df.columns:
            # è¡¨è¨˜æºã‚Œã«å¯¾å¿œï¼ˆéƒ¨åˆ†ä¸€è‡´ã§åˆ¤å®šï¼‰
            kao_planet_rows = existing_df[
                existing_df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'].str.contains('èŠ±ç‹|ãƒ—ãƒ©ãƒãƒƒãƒˆ|KAO|PLANET', 
                                                        case=False, 
                                                        na=False, 
                                                        regex=True)
            ]
            existing_kao_planet_jans = set(kao_planet_rows['æ–°JANã‚³ãƒ¼ãƒ‰'].dropna())
            print(f"  ç´¯ç©å†…ã®èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆç”±æ¥JAN: {len(existing_kao_planet_jans)}ä»¶")
        else:
            print("  âš ï¸ ç´¯ç©ãƒªã‚¹ãƒˆã«ã€Œãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã€åˆ—ãŒã‚ã‚Šã¾ã›ã‚“ï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒãƒãƒ³ã‚°çµæœã‹ã‚‰èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆé‡è¤‡ã‚’å‰Šé™¤ ==========
    if not matching_df.empty and existing_kao_planet_jans:
        before_count = len(matching_df)
        matching_df = matching_df[~matching_df['æ–°JANã‚³ãƒ¼ãƒ‰'].isin(existing_kao_planet_jans)].copy()
        removed_count = before_count - len(matching_df)
        if removed_count > 0:
            print(f"  âœ‚ï¸ ãƒãƒƒãƒãƒ³ã‚°â†’ç´¯ç©å†…èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆé‡è¤‡å‰Šé™¤: {removed_count}ä»¶")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—3: ä»Šé€±ã®èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆã¨ãƒãƒƒãƒãƒ³ã‚°ã®é‡è¤‡å‰Šé™¤ ==========
    if not kao_planet_df.empty and not matching_df.empty:
        # æ—§JANãƒ»æ–°JANã®ä¸¡æ–¹ã§çªåˆ
        kao_planet_old_jans = set(kao_planet_df['æ—§JANã‚³ãƒ¼ãƒ‰'].dropna())
        kao_planet_new_jans = set(kao_planet_df['æ–°JANã‚³ãƒ¼ãƒ‰'].dropna())
        
        before_count = len(matching_df)
        
        # æ–°JANã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚‚ã®ã‚’å‰Šé™¤
        matching_df = matching_df[~matching_df['æ–°JANã‚³ãƒ¼ãƒ‰'].isin(kao_planet_new_jans)].copy()
        
        # æ—§JANã§é‡è¤‡ã—ã¦ã„ã‚‹ã‚‚ã®ã‚’å‰Šé™¤
        matching_df = matching_df[~matching_df['æ—§JANã‚³ãƒ¼ãƒ‰'].isin(kao_planet_old_jans)].copy()
        
        removed_count = before_count - len(matching_df)
        if removed_count > 0:
            print(f"  âœ‚ï¸ ãƒãƒƒãƒãƒ³ã‚°â†’ä»Šé€±èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆé‡è¤‡å‰Šé™¤: {removed_count}ä»¶")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—4: 3ã¤ã®ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆï¼ˆå„ªå…ˆé †ä½é †ï¼‰ ==========
    all_data = pd.concat([
        existing_df,      # 1ä½ï¼ˆæœ€å„ªå…ˆï¼‰
        kao_planet_df,    # 2ä½
        matching_df       # 3ä½ï¼ˆæœ€ä½å„ªå…ˆï¼‰
    ], ignore_index=True)
    
    print(f"  çµ±åˆå¾Œ: {len(all_data)}ä»¶")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—5: æ–°JANã‚³ãƒ¼ãƒ‰ã§é‡è¤‡å‰Šé™¤ ==========
    before_dedup = len(all_data)
    all_data = all_data.drop_duplicates(subset=['æ–°JANã‚³ãƒ¼ãƒ‰'], keep='first')
    after_dedup = len(all_data)
    
    removed_by_dedup = before_dedup - after_dedup
    if removed_by_dedup > 0:
        print(f"  ğŸ—‘ï¸ æ–°JANé‡è¤‡å‰Šé™¤: {removed_by_dedup}ä»¶")
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—6: æ—§JAN=æ–°JANã®ã‚‚ã®ã‚’å‰Šé™¤ ==========
    before_same_jan = len(all_data)
    all_data = all_data[all_data['æ—§JANã‚³ãƒ¼ãƒ‰'] != all_data['æ–°JANã‚³ãƒ¼ãƒ‰']].copy()
    after_same_jan = len(all_data)
    
    removed_same_jan = before_same_jan - after_same_jan
    if removed_same_jan > 0:
        print(f"  ğŸ”„ åŒä¸€JANå‰Šé™¤: {removed_same_jan}ä»¶")
    
    print(f"  âœ… æœ€çµ‚ä»¶æ•°: {len(all_data)}ä»¶")
    
    return all_data


# ========================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================================================

def main():
    """
    ç´¯ç©ãƒªã‚¹ãƒˆçµ±åˆãƒ¡ã‚¤ãƒ³å‡¦ç†
    """
    print("=" * 60)
    print("ç´¯ç©ãƒªã‚¹ãƒˆçµ±åˆãƒ—ãƒ­ã‚°ãƒ©ãƒ  - é€±æ¬¡æ›´æ–°ï¼ˆå®Œå…¨ç‰ˆï¼‰")
    print("=" * 60)
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—1: ç´¯ç©ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ ==========
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—1ã€‘ç´¯ç©ãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿")
    
    root = tk.Tk()
    root.withdraw()
    
    if messagebox.askyesno("ç´¯ç©ãƒªã‚¹ãƒˆ", "å‰é€±ã®ç´¯ç©ãƒªã‚¹ãƒˆãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿ\nï¼ˆåˆå›ã¯ã€Œã„ã„ãˆã€ï¼‰"):
        existing_path = filedialog.askopenfilename(
            title="å‰é€±ã®ç´¯ç©ãƒªã‚¹ãƒˆã‚’é¸æŠ",
            filetypes=[
                ("Excel", "*.xlsx *.xls *.xlsm"),
                ("CSV", "*.csv"),
                ("ã™ã¹ã¦", "*.*")
            ]
        )
        
        if existing_path:
            try:
                existing_df = load_file_flexible(existing_path)
                # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¯æ­£è¦åŒ–ã—ãªã„ï¼ˆãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æƒ…å ±ã‚’ä¿æŒï¼‰
                existing_df = clean_jan_codes(existing_df)
                
                # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ—ãŒãªã„å ´åˆã¯è¿½åŠ 
                if 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹' not in existing_df.columns:
                    print("  âš ï¸ ã€Œãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã€åˆ—ãŒãªã„ãŸã‚è¿½åŠ ã—ã¾ã™")
                    existing_df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'] = 'ç´¯ç©ï¼ˆæ—§ç‰ˆï¼‰'
                
                print(f"âœ… ç´¯ç©ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿: {len(existing_df)}ä»¶")
                
            except Exception as e:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ç´¯ç©ãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—:\n{e}")
                root.destroy()
                return
        else:
            existing_df = pd.DataFrame()
    else:
        existing_df = pd.DataFrame()
        print("ğŸ“‚ æ–°è¦ä½œæˆãƒ¢ãƒ¼ãƒ‰")
    
    root.destroy()
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—2: ãƒãƒƒãƒãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿ ==========
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—2ã€‘ãƒãƒƒãƒãƒ³ã‚°çµæœã®èª­ã¿è¾¼ã¿")
    
    root = tk.Tk()
    root.withdraw()
    
    messagebox.showinfo("é¸æŠ", "ä»Šé€±ã®ãƒãƒƒãƒãƒ³ã‚°çµæœãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„\nï¼ˆäººé–“ãŒä¿®æ­£æ¸ˆã¿ã®ã‚‚ã®ï¼‰")
    
    matching_path = filedialog.askopenfilename(
        title="ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’é¸æŠ",
        filetypes=[
            ("Excel", "*.xlsx *.xls *.xlsm"),
            ("CSV", "*.csv"),
            ("TSV", "*.tsv"),
            ("ã™ã¹ã¦", "*.*")
        ]
    )
    
    root.destroy()
    
    if not matching_path:
        messagebox.showwarning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", "ãƒãƒƒãƒãƒ³ã‚°çµæœãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    try:
        matching_df = load_file_flexible(matching_path)
        matching_df = normalize_columns(matching_df, 'ãƒãƒƒãƒãƒ³ã‚°')
        matching_df = clean_jan_codes(matching_df)
        print(f"âœ… ãƒãƒƒãƒãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿: {len(matching_df)}ä»¶")
    except Exception as e:
        messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒãƒƒãƒãƒ³ã‚°çµæœèª­ã¿è¾¼ã¿å¤±æ•—:\n{e}")
        return
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—3: èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå·®ã—æ›¿ãˆãƒªã‚¹ãƒˆèª­ã¿è¾¼ã¿ ==========
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—3ã€‘èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå·®ã—æ›¿ãˆãƒªã‚¹ãƒˆã®èª­ã¿è¾¼ã¿")
    
    root = tk.Tk()
    root.withdraw()
    
    kao_planet_df = pd.DataFrame()
    
    # èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆã¯ä»»æ„ï¼ˆåŠå¹´ã«1å›ç¨‹åº¦ï¼‰
    choice = messagebox.askquestion(
        "èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆ", 
        "ä»Šé€±ã€èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆã®å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆã¯ã‚ã‚Šã¾ã™ã‹ï¼Ÿ\n"
        "ï¼ˆåŠå¹´ã«1å›ç¨‹åº¦ã®æ›´æ–°ï¼‰\n\n"
        "ã€Œã¯ã„ã€â†’ ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ\n"
        "ã€Œã„ã„ãˆã€â†’ ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒãƒƒãƒãƒ³ã‚°ã®ã¿çµ±åˆï¼‰",
        icon='question'
    )
    
    if choice == 'yes':
        kao_planet_path = filedialog.askopenfilename(
            title="èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå·®ã—æ›¿ãˆãƒªã‚¹ãƒˆã‚’é¸æŠ",
            filetypes=[
                ("Excel", "*.xlsx *.xls"),
                ("CSV", "*.csv"),
                ("ã™ã¹ã¦", "*.*")
            ]
        )
        
        if kao_planet_path:
            try:
                kao_planet_df = load_file_flexible(kao_planet_path)
                kao_planet_df = normalize_columns(kao_planet_df, 'èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆ')
                kao_planet_df = clean_jan_codes(kao_planet_df)
                print(f"âœ… èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆèª­ã¿è¾¼ã¿: {len(kao_planet_df)}ä»¶")
            except Exception as e:
                messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆèª­ã¿è¾¼ã¿å¤±æ•—:\n{e}")
                root.destroy()
                return
        else:
            print("ğŸ“‚ ãƒ•ã‚¡ã‚¤ãƒ«æœªé¸æŠï¼ˆã‚¹ã‚­ãƒƒãƒ—ï¼‰")
    else:
        print("ğŸ“‚ èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆãªã—ï¼ˆãƒãƒƒãƒãƒ³ã‚°ã®ã¿çµ±åˆï¼‰")
    
    root.destroy()
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—4: å‡ºåŠ›å…ˆé¸æŠ ==========
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—4ã€‘å‡ºåŠ›å…ˆã®é¸æŠ")
    
    root = tk.Tk()
    root.withdraw()
    
    output_dir = filedialog.askdirectory(title="ä¿å­˜å…ˆãƒ•ã‚©ãƒ«ãƒ€ã‚’é¸æŠ")
    
    root.destroy()
    
    if not output_dir:
        messagebox.showwarning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", "ä¿å­˜å…ˆãŒé¸æŠã•ã‚Œã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    output_dir = Path(output_dir)
    
    # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ä»˜ããƒ•ã‚¡ã‚¤ãƒ«å
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
    output_csv = output_dir / f"ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ_{timestamp}.csv"
    output_excel = output_dir / f"ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ_{timestamp}.xlsx"
    
    # æœ€æ–°ç‰ˆï¼ˆã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ãªã—ï¼‰
    latest_csv = output_dir / "ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ_æœ€æ–°.csv"
    latest_excel = output_dir / "ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ_æœ€æ–°.xlsx"
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—5: ãƒ‡ãƒ¼ã‚¿çµ±åˆ ==========
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—5ã€‘ãƒ‡ãƒ¼ã‚¿çµ±åˆãƒ»é‡è¤‡å‰Šé™¤")
    
    final_df = remove_duplicates_advanced(existing_df, kao_planet_df, matching_df)
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—6: ä¿å­˜ ==========
    print("\nã€ã‚¹ãƒ†ãƒƒãƒ—6ã€‘ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜")
    
    try:
        # CSVä¿å­˜ï¼ˆcp932ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼‰
        print(f"ğŸ’¾ CSVä¿å­˜ä¸­ï¼ˆcp932ï¼‰: {output_csv.name}")
        final_df.to_csv(output_csv, index=False, encoding='cp932', errors='replace')
        final_df.to_csv(latest_csv, index=False, encoding='cp932', errors='replace')
        
        # Excelä¿å­˜
        print(f"ğŸ’¾ Excelä¿å­˜ä¸­: {output_excel.name}")
        final_df.to_excel(output_excel, index=False, engine='openpyxl')
        final_df.to_excel(latest_excel, index=False, engine='openpyxl')
        
        print("âœ… ä¿å­˜å®Œäº†")
        
    except Exception as e:
        messagebox.showerror("ã‚¨ãƒ©ãƒ¼", f"ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å¤±æ•—:\n{e}")
        return
    
    # ========== ã‚¹ãƒ†ãƒƒãƒ—7: å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ ==========
    
    # ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã®é›†è¨ˆ
    source_counts = final_df['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'].value_counts().to_dict() if 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹' in final_df.columns else {}
    
    summary = f
    """
    çµ±åˆå‡¦ç†å®Œäº†
    
    ã€ç´¯ç©ãƒ‡ãƒ¼ã‚¿ã€‘
    ç·ä»¶æ•°: {len(final_df)}ä»¶
    ã€å†…è¨³ã€‘
    æ—¢å­˜ç´¯ç©: {len(existing_df)}ä»¶
    èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆï¼ˆä»Šé€±ï¼‰: {len(kao_planet_df)}ä»¶
    ãƒãƒƒãƒãƒ³ã‚°ï¼ˆä»Šé€±ï¼‰: {len(matching_df)}ä»¶
    
    ã€ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹åˆ¥ã€‘
    {chr(10).join([f'{k}: {v}ä»¶' for k, v in source_counts.items()])}
    
    ã€ä¿å­˜å…ˆã€‘
    {output_dir}
    
    ã€ãƒ•ã‚¡ã‚¤ãƒ«ã€‘
    - {output_csv.name}ï¼ˆcp932ï¼‰
    - {output_excel.name}
    - ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ_æœ€æ–°.csv
    - ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ_æœ€æ–°.xlsx
    
    """
    
    print(summary)
    messagebox.showinfo("å®Œäº†", summary)


# ========================================================================
# å®Ÿè¡Œ
# ========================================================================

if __name__ == "__main__":
    main()