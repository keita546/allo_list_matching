# -*- coding: utf-8 -*-
"""
çµ±åˆãƒã‚¹ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå®Œå…¨ç‰ˆãƒ»1ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰
ãƒãƒƒãƒãƒ³ã‚°å‡¦ç† + èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç† + ç´¯ç©ç®¡ç†

Author: HIBI KEITA
Version: 1.0
"""

import pandas as pd
from pathlib import Path
import tkinter as tk
from tkinter import filedialog, messagebox
import os
from datetime import datetime
from fuzzywuzzy import fuzz
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Alignment, Font
from openpyxl.utils.dataframe import dataframe_to_rows

# win32comï¼ˆExcelä¿®å¾©ç”¨ï¼‰
try:
    import win32com.client as win32
    import pythoncom
    WIN32COM_AVAILABLE = True
except ImportError:
    WIN32COM_AVAILABLE = False


# ========================================================================
# å…±é€šãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£é–¢æ•°
# ========================================================================

def calculate_similarity(s1: str, s2: str) -> float:
    """æ–‡å­—åˆ—ã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆ0.0ã€œ1.0ï¼‰"""
    if pd.isna(s1) or pd.isna(s2):
        return 0.0
    return fuzz.ratio(str(s1), str(s2)) / 100.0


def get_weight_range(weight):
    """ç›®ä»˜ã‹ã‚‰è¨±å®¹ç¯„å›²ã‚’è¨ˆç®—ï¼ˆ90%~110%ï¼‰"""
    try:
        w = float(weight)
        return w * 0.9, w * 1.1
    except (ValueError, TypeError):
        return None, None


# ========================================================================
# ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ========================================================================

def load_data_for_matching(file_path: str, suffix: str) -> pd.DataFrame:
    """
    ãƒãƒƒãƒãƒ³ã‚°ç”¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ - ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤åˆ¥å¼·åŒ–ç‰ˆ
    """
    p = Path(file_path)
    ext = p.suffix.lower()
    df = pd.DataFrame()
    
    # ã€Excelãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã€‘: Excelã¯read_csvã§ã¯èª­ã‚ãªã„ã‹ã‚‰ã€åˆ¥ã®é–¢æ•°ã§å‡¦ç†ã‚’åˆ†ã‘ã‚‹ã‚ˆ
    if ext in ['.xlsx', '.xls', '.xlsm']:
        try:
            # openpyxlã‚¨ãƒ³ã‚¸ãƒ³ã§Excelã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ã‚ˆ
            df = pd.read_excel(file_path, engine='openpyxl')
            # èª­ã¿è¾¼ã¿æˆåŠŸã—ãŸã‚‰ã™ãã«å¾Œç¶šå‡¦ç†ã¸ç§»ã‚‹ã‚ˆ
            return df.replace('NULL', pd.NA).add_suffix(suffix)
        except Exception as e:
            # Excelç‰¹æœ‰ã®ã‚¨ãƒ©ãƒ¼ãŒå‡ºãŸã‚‰ã€ã™ãã«ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«çŸ¥ã‚‰ã›ã‚‹ã‚ˆ
            raise ValueError(f"Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")

    # ã€CSV/TSV/TXTãƒ•ã‚¡ã‚¤ãƒ«å‡¦ç†ã€‘: ãƒ†ã‚­ã‚¹ãƒˆãƒ™ãƒ¼ã‚¹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã™ã‚‹ã‚ˆ
    read_params = {
        '.csv': {'delimiter': ','},
        '.tsv': {'delimiter': '\t'}, # ã“ã“ãŒTSVå¯¾å¿œã®ã‚­ãƒ¢ï¼åŒºåˆ‡ã‚Šæ–‡å­—ã‚’ã‚¿ãƒ–ã«è¨­å®šã—ã¦ã‚‹ã‚ˆ
        '.txt': {'delimiter': '\t'},
    }
    
    # ã‚µãƒãƒ¼ãƒˆå¤–ã®æ‹¡å¼µå­ãªã‚‰ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™ã‚ˆ
    if ext not in read_params:
        raise ValueError(f"ã‚µãƒãƒ¼ãƒˆå¤–ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼:{ext}")

    # æ—¥æœ¬èªãƒ‡ãƒ¼ã‚¿ã§ã‚ˆãã‚ã‚‹ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã®ãƒªã‚¹ãƒˆã‚’å®šç¾©ã—ã¦ã„ã‚‹ã‚ˆ
    encodings = ['utf-8', 'shift_jis', 'cp932', 'euc_jp']
    
    common_args = {
        'delimiter': read_params[ext]['delimiter'],
        'on_bad_lines': 'skip' # ä¸æ­£ãªè¡Œã¯ã‚¹ã‚­ãƒƒãƒ—ã—ã¦ã€å‡¦ç†ã‚’ä¸­æ–­ã•ã›ãªã„ã‚ˆã†ã«ã—ã¦ã‚‹ã‚ˆ
    }

    # å®šç¾©ã—ãŸã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’é †ç•ªã«è©¦ã—ã¦ã„ã‚‹ã‚ˆ
    for encoding in encodings:
        try:
            # è©¦è¡ŒéŒ¯èª¤ã§ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚“ã§ã„ã‚‹ã‚ˆ
            df = pd.read_csv(file_path, encoding=encoding, **common_args)
            # print(f"âœ… {ext}ã‚’'{encoding}'ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã§èª­ã¿è¾¼ã¿æˆåŠŸã€‚")
            break # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—ã‚’æŠœã‘ã‚‹ã‚ˆ
        except UnicodeDecodeError:
            continue # ãƒ‡ã‚³ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼ãªã‚‰æ¬¡ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’è©¦ã™ã‚ˆ
        except Exception as e:
            raise Exception(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­ã«äºˆæœŸã›ã¬ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
    else:
        # ã™ã¹ã¦å¤±æ•—ã—ãŸã‚‰ã‚¨ãƒ©ãƒ¼ã‚’å‡ºã™ã‚ˆ
        raise UnicodeDecodeError(f"ã™ã¹ã¦ã®ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°({', '.join(encodings)})ã§'{ext}'ãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸã€‚")
    
    # æ¬ æå€¤ã®çµ±ä¸€å‡¦ç†ã‚’ã—ã¦ã„ã‚‹ã‚ˆ
    df = df.replace('NULL', pd.NA)
    
    # å¾Œç¶šå‡¦ç†ã«å¿…è¦ãªå¿…é ˆã‚«ãƒ©ãƒ ã®ãƒªã‚¹ãƒˆã ã‚ˆ
    required_cols = [
        'ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚³ãƒ¼ãƒ‰', 'ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰', 'æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—)',
        'ç›®ä»˜', 'ãƒ–ãƒ©ãƒ³ãƒ‰åç§°', 'æ¨™æº–åˆ†é¡å(ã‚¯ãƒ©ã‚¹)',
        'å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰', 'JANã‚³ãƒ¼ãƒ‰', 'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°',
    ]
    
    # å¿…é ˆã‚«ãƒ©ãƒ ãŒãªã„å ´åˆã¯ã€NAã‚«ãƒ©ãƒ ã‚’è¿½åŠ ã—ã¦ã‚¨ãƒ©ãƒ¼ã‚’å›é¿ã—ã¦ã„ã‚‹ã‚ˆ
    for col in required_cols:
        if col not in df.columns:
            df[col] = pd.NA
    
    # æ–°æ—§ãƒã‚¹ã‚¿ã®åŒºåˆ¥ã‚’ã¤ã‘ã‚‹ãŸã‚ã«æ¥å°¾è¾ã‚’è¿½åŠ ã—ã¦ã„ã‚‹ã‚ˆ
    df = df.add_suffix(suffix)
    return df


def clean_initial_data(df: pd.DataFrame, suffix: str) -> pd.DataFrame:
    """åˆæœŸãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°"""
    maker_col = f'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°{suffix}'
    brand_col = f'ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰{suffix}'
    type_col = f'æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—){suffix}'
    
    before_count = len(df)
    df_cleaned = df[
        ~(df[maker_col].isna() & df[brand_col].isna() & df[type_col].isna())
    ].copy()
    
    after_count = len(df_cleaned)
    print(f"ã€ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°{suffix}ã€‘{before_count}è¡Œ â†’ {after_count}è¡Œ")
    
    return df_cleaned


def preprocess_old_data(df_old: pd.DataFrame):
    """æ—§ãƒã‚¹ã‚¿äº‹å‰å‡¦ç†ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹åŒ–ï¼‰"""
    df_processed = df_old.copy()
    df_processed['ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ—§'] = df_processed['ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ—§'].astype(str).str.strip()
    df_processed['ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰_æ—§'] = df_processed['ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰_æ—§'].astype(str).str.strip()
    df_processed['æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—)_æ—§'] = df_processed['æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—)_æ—§'].astype(str).str.strip()
    df_processed['ç›®ä»˜_æ—§_float'] = pd.to_numeric(df_processed['ç›®ä»˜_æ—§'], errors='coerce')
    
    df_brands_indexed = df_processed.set_index('ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰_æ—§')
    df_multi_indexed = df_processed.set_index(['ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ—§', 'æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—)_æ—§'])
    
    return df_processed, df_brands_indexed, df_multi_indexed


def find_best_match(new_row, df_old_processed, df_old_brands_indexed, df_old_multi_indexed, df_old_original):
    """1è¡Œã®æ–°å“ã«å¯¾ã—ã¦æœ€é©ãªæ—§å“ã‚’æ¤œç´¢"""
    new_maker_name = str(new_row.get('ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ–°')).strip() if pd.notna(new_row.get('ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ–°')) else None
    new_brand = str(new_row.get('ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰_æ–°')).strip() if pd.notna(new_row.get('ãƒ–ãƒ©ãƒ³ãƒ‰ã‚³ãƒ¼ãƒ‰_æ–°')) else None
    new_type = str(new_row.get('æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—)_æ–°')).strip() if pd.notna(new_row.get('æ¨™æº–åˆ†é¡ã‚³ãƒ¼ãƒ‰(ã‚¿ã‚¤ãƒ—)_æ–°')) else None
    new_weight = new_row.get('ç›®ä»˜_æ–°')
    new_name = new_row.get('å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ–°')
    
    if new_maker_name == 'nan': new_maker_name = None
    if new_brand == 'nan': new_brand = None
    if new_type == 'nan': new_type = None
    
    skip_reasons = []
    matching_old = None
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ãƒ–ãƒ©ãƒ³ãƒ‰ã‚ã‚Š
    if new_brand:
        try:
            matching_old = df_old_brands_indexed.loc[new_brand].copy()
            if isinstance(matching_old, pd.Series):
                matching_old = matching_old.to_frame().T
        except KeyError:
            matching_old = df_old_processed.iloc[0:0].copy()
        
        if matching_old.empty:
            return {'ç…§åˆçµæœ': 'å€™è£œãªã—ï¼ˆãƒ–ãƒ©ãƒ³ãƒ‰ä¸ä¸€è‡´ï¼‰', 'æœ€é«˜é¡ä¼¼åº¦': 0.0, 'åˆ¤å®š': 'âœ•', 
                    'å€™è£œ': '', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': '', 'å€™è£œã‚ã‚Š': False}
        
        if pd.notna(new_weight):
            min_w, max_w = get_weight_range(new_weight)
            if min_w and max_w:
                weight_filtered = matching_old[
                    (matching_old['ç›®ä»˜_æ—§_float'] >= min_w) & 
                    (matching_old['ç›®ä»˜_æ—§_float'] <= max_w)
                ]
                if weight_filtered.empty:
                    return {'ç…§åˆçµæœ': 'å€™è£œãªã—ï¼ˆç›®ä»˜ç¯„å›²å¤–ï¼‰', 'æœ€é«˜é¡ä¼¼åº¦': 0.0, 'åˆ¤å®š': 'âœ•',
                            'å€™è£œ': '', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': '', 'å€™è£œã‚ã‚Š': False}
                matching_old = weight_filtered
        else:
            skip_reasons.append('ç›®ä»˜ã‚¹ã‚­ãƒƒãƒ—')
    
    # ãƒ‘ã‚¿ãƒ¼ãƒ³2: ãƒ–ãƒ©ãƒ³ãƒ‰ãªã— â†’ ãƒ¡ãƒ¼ã‚«ãƒ¼+ã‚¿ã‚¤ãƒ—
    elif new_maker_name and new_type:
        try:
            matching_old = df_old_multi_indexed.loc[(new_maker_name, new_type)].copy()
            if isinstance(matching_old, pd.Series):
                matching_old = matching_old.to_frame().T
        except KeyError:
            matching_old = df_old_processed.iloc[0:0].copy()
        
        if matching_old.empty:
            return {'ç…§åˆçµæœ': 'å€™è£œãªã—ï¼ˆãƒ¡ãƒ¼ã‚«ãƒ¼åç§°+ã‚¿ã‚¤ãƒ—ä¸ä¸€è‡´ï¼‰', 'æœ€é«˜é¡ä¼¼åº¦': 0.0, 'åˆ¤å®š': 'âœ•',
                    'å€™è£œ': '', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': '', 'å€™è£œã‚ã‚Š': False}
        
        if pd.notna(new_weight):
            min_w, max_w = get_weight_range(new_weight)
            if min_w and max_w:
                weight_filtered = matching_old[
                    (matching_old['ç›®ä»˜_æ—§_float'] >= min_w) & 
                    (matching_old['ç›®ä»˜_æ—§_float'] <= max_w)
                ]
                if weight_filtered.empty:
                    return {'ç…§åˆçµæœ': 'å€™è£œãªã—ï¼ˆç›®ä»˜ç¯„å›²å¤–ï¼‰', 'æœ€é«˜é¡ä¼¼åº¦': 0.0, 'åˆ¤å®š': 'âœ•',
                            'å€™è£œ': '', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': '', 'å€™è£œã‚ã‚Š': False}
                matching_old = weight_filtered
        else:
            skip_reasons.append('ç›®ä»˜ã‚¹ã‚­ãƒƒãƒ—')
    else:
        return {'ç…§åˆçµæœ': 'å€™è£œãªã—ï¼ˆã‚­ãƒ¼ã‚³ãƒ¼ãƒ‰ä¸è¶³ï¼‰', 'æœ€é«˜é¡ä¼¼åº¦': 0.0, 'åˆ¤å®š': 'âœ•',
                'å€™è£œ': '', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': '', 'å€™è£œã‚ã‚Š': False}
    
    # åç§°ä¸€è‡´ã§æœ€é«˜é¡ä¼¼åº¦ã‚’é¸æŠ
    candidates = matching_old[['å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§', 'JANã‚³ãƒ¼ãƒ‰_æ—§']].drop_duplicates()
    
    if candidates.empty:
        return {'ç…§åˆçµæœ': 'å€™è£œãªã—ï¼ˆåç§°ä¸€è‡´ãªã—ï¼‰', 'æœ€é«˜é¡ä¼¼åº¦': 0.0, 'åˆ¤å®š': 'âœ•',
                'å€™è£œ': '', 'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': 'ã€'.join(skip_reasons) if skip_reasons else '', 'å€™è£œã‚ã‚Š': False}
    
    similarities = [
        (calculate_similarity(new_name, row['å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§']), 
         row['å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§'], 
         row['JANã‚³ãƒ¼ãƒ‰_æ—§'])
        for _, row in candidates.iterrows()
    ]
    similarities.sort(key=lambda x: x[0], reverse=True)
    
    best_score, best_name, best_jan = similarities[0]
    best_old_row = df_old_original[df_old_original['JANã‚³ãƒ¼ãƒ‰_æ—§'] == best_jan].iloc[0]
    
    if best_score >= 0.8:
        result = 'é«˜é¡ä¼¼åº¦å€™è£œã‚ã‚Š (80%ä»¥ä¸Š)'
        judgment = 'â—‹'
    else:
        result = 'ä½é¡ä¼¼åº¦ (80%æœªæº€ãƒ»è¦æ‰‹å‹•ç¢ºèª)'
        judgment = 'âœ•'
    
    return {
        'ç…§åˆçµæœ': result,
        'æœ€é«˜é¡ä¼¼åº¦': best_score,
        'åˆ¤å®š': judgment,
        'å€™è£œ': f"{best_name}({best_score:.1%})",
        'ã‚¹ã‚­ãƒƒãƒ—ç†ç”±': 'ã€'.join(skip_reasons) if skip_reasons else '',
        'å€™è£œã‚ã‚Š': True,
        'JANã‚³ãƒ¼ãƒ‰_æ—§': best_old_row.get('JANã‚³ãƒ¼ãƒ‰_æ—§', ''),
        'å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§': best_old_row.get('å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§', ''),
        'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ—§': best_old_row.get('ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ—§', ''),
        'æ¨™æº–åˆ†é¡(ã‚¯ãƒ©ã‚¹)_æ—§': best_old_row.get('æ¨™æº–åˆ†é¡å(ã‚¯ãƒ©ã‚¹)_æ—§', ''),
        'ãƒ–ãƒ©ãƒ³ãƒ‰åç§°_æ—§': best_old_row.get('ãƒ–ãƒ©ãƒ³ãƒ‰åç§°_æ—§', ''),
        'ç›®ä»˜_æ—§': best_old_row.get('ç›®ä»˜_æ—§', ''),
        'ç™ºå£²æ—¥_æ—§': best_old_row.get('ç™ºå£²æ—¥_æ—§', ''),
    }


def run_matching_process(old_path: str, new_path: str) -> pd.DataFrame:
    """ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†å®Ÿè¡Œï¼ˆçµ±åˆç”¨ï¼‰"""
    print("\nğŸ“Š ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†é–‹å§‹...")
    
    df_new = load_data_for_matching(new_path, '_æ–°')
    df_old = load_data_for_matching(old_path, '_æ—§')
    
    df_new = clean_initial_data(df_new, '_æ–°')
    df_old = clean_initial_data(df_old, '_æ—§')
    
    if 'ç™ºå£²æ—¥_æ—§' not in df_old.columns:
        df_old['ç™ºå£²æ—¥_æ—§'] = pd.NA
    
    print("æ—§ãƒã‚¹ã‚¿å‰å‡¦ç†ä¸­...")
    df_old_processed, df_old_brands_indexed, df_old_multi_indexed = preprocess_old_data(df_old)
    
    print(f"çªåˆå‡¦ç†é–‹å§‹...ï¼ˆ{len(df_new)}ä»¶ï¼‰")
    
    results = []
    for idx, new_row in df_new.iterrows():
        if idx % 100 == 0:
            print(f"å‡¦ç†ä¸­... {idx}/{len(df_new)}")
        
        result = find_best_match(new_row, df_old_processed, df_old_brands_indexed, 
                                df_old_multi_indexed, df_old)
        results.append(result)
    
    analysis_result = pd.DataFrame(results)
    final_df = pd.concat([df_new.reset_index(drop=True), analysis_result], axis=1)
    
    final_df = final_df.rename(columns={
        'æ¨™æº–åˆ†é¡å(ã‚¯ãƒ©ã‚¹)_æ–°': 'æ¨™æº–åˆ†é¡(ã‚¯ãƒ©ã‚¹)_æ–°',
    }, errors='ignore')
    
    # å€™è£œã‚ã‚Šã®ã¿ãƒ•ã‚£ãƒ«ã‚¿
    if 'å€™è£œã‚ã‚Š' in final_df.columns:
        final_df = final_df[final_df['å€™è£œã‚ã‚Š'] == True].copy()
        final_df = final_df.drop('å€™è£œã‚ã‚Š', axis=1)
    
    print(f"âœ… ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†å®Œäº†: {len(final_df)}ä»¶")
    return final_df


# ========================================================================
# èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ========================================================================

def repair_and_resave_excel(file_path):
    """Excelè‡ªå‹•ä¿®å¾©"""
    if not WIN32COM_AVAILABLE:
        return False
    
    p_file = Path(file_path)
    print(f"ğŸ› ï¸ Excelãƒ•ã‚¡ã‚¤ãƒ«ä¿®å¾©ä¸­: {p_file.name}")
    
    excel = None
    try:
        pythoncom.CoInitialize()
    except:
        pass
    
    try:
        excel = win32.Dispatch('Excel.Application')
        excel.Visible = False
        excel.DisplayAlerts = False
        
        workbook = excel.Workbooks.Open(str(p_file.resolve()), UpdateLinks=False, ReadOnly=False)
        workbook.Save()
        workbook.Close(SaveChanges=False)
        
        print(f"âœ… ä¿®å¾©å®Œäº†: {p_file.name}")
        return True
    except Exception as e:
        print(f"âŒ ä¿®å¾©å¤±æ•—: {e}")
        return False
    finally:
        if excel:
            excel.Quit()
        try:
            pythoncom.CoUninitialize()
        except:
            pass


def load_with_repair(path, **read_excel_kwargs):
    """ä¿®å¾©æ©Ÿèƒ½ä»˜ãExcelèª­ã¿è¾¼ã¿"""
    p_file = Path(path)
    
    try:
        df = pd.read_excel(p_file, engine='openpyxl', **read_excel_kwargs)
        return df
    except Exception as e:
        print(f"âš ï¸ é€šå¸¸èª­ã¿è¾¼ã¿å¤±æ•—: {e}")
        
        if WIN32COM_AVAILABLE:
            print("ğŸ”§ ä¿®å¾©ã‚’è©¦ã¿ã¦ã„ã¾ã™...")
            if repair_and_resave_excel(p_file):
                try:
                    df = pd.read_excel(p_file, engine='openpyxl', **read_excel_kwargs)
                    return df
                except Exception as retry_e:
                    print(f"âŒ ä¿®å¾©å¾Œã‚‚èª­ã¿è¾¼ã¿å¤±æ•—: {retry_e}")
                    raise
        raise


def load_kao(path):
    """èŠ±ç‹ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    df = load_with_repair(
        path,
        usecols=[6, 14, 41, 43],
        skiprows=5,
        header=None,
        dtype={14: str, 41: str}
    )
    
    df.columns = ['æ–°å•†å“å', 'æ–°JAN', 'æ—§JAN', 'æ—§å•†å“å']
    df = df.dropna(subset=['æ—§JAN', 'æ–°JAN'])[['æ—§JAN', 'æ—§å•†å“å', 'æ–°JAN', 'æ–°å•†å“å']]
    df['å‚™è€ƒ'] = path.name
    return df


def clean_planet(df, mode):
    """ãƒ—ãƒ©ãƒãƒƒãƒˆãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°"""
    df.columns = df.columns.str.replace('ï¼ªï¼¡ï¼®', 'JAN')
    
    if mode == 'discontinue':
        required_cols = ['JANã‚³ãƒ¼ãƒ‰', 'æ–°JANã‚³ãƒ¼ãƒ‰', 'å»ƒç•ªäºˆå®šå“', 'æ–°å•†å“å']
        df = df.dropna(subset=required_cols)
        
        return df.rename(columns={
            'JANã‚³ãƒ¼ãƒ‰': 'æ—§JAN',
            'æ–°JANã‚³ãƒ¼ãƒ‰': 'æ–°JAN',
            'å»ƒç•ªäºˆå®šå“': 'æ—§å•†å“å',
            'æ–°å•†å“å': 'æ–°å•†å“å'
        })[['æ—§JAN', 'æ—§å•†å“å', 'æ–°JAN', 'æ–°å•†å“å']]
    else:
        df = df.dropna(subset=['JANã‚³ãƒ¼ãƒ‰', 'æ—§JANã‚³ãƒ¼ãƒ‰'])
        return df.rename(columns={
            'æ—§JANã‚³ãƒ¼ãƒ‰': 'æ—§JAN',
            'JANã‚³ãƒ¼ãƒ‰': 'æ–°JAN',
            'å•†å“åå…¨è§’': 'æ–°å•†å“å'
        })[['æ—§JAN', 'æ–°JAN', 'æ–°å•†å“å']]


def extract_unmatched(new_df, old_df):
    """ç´”ç²‹æ–°è¦å“æŠ½å‡º"""
    add = new_df[~new_df['æ–°JAN'].isin(old_df['æ—§JAN'])].copy()
    add['æ—§å•†å“å'] = ''
    return add[['æ—§JAN', 'æ—§å•†å“å', 'æ–°JAN', 'æ–°å•†å“å']]


def exclude_kao(df, is_kao_col):
    """èŠ±ç‹ãƒ‡ãƒ¼ã‚¿é™¤å¤–"""
    return df[~df[is_kao_col].astype(str).str.startswith('4901301') & 
              ~df[is_kao_col].astype(str).str.contains('èŠ±ç‹æ ªå¼ä¼šç¤¾')]


def finalize_kao_planet(df):
    """èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆãƒ‡ãƒ¼ã‚¿æœ€çµ‚ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    df = df.rename(columns={'æ—§JAN': 'æ—§JANã‚³ãƒ¼ãƒ‰', 'æ–°JAN': 'æ–°JANã‚³ãƒ¼ãƒ‰'})
    
    for col in ['æ—§JANã‚³ãƒ¼ãƒ‰', 'æ–°JANã‚³ãƒ¼ãƒ‰']:
        df[col] = (df[col].astype(str)
                   .str.replace(r'\D+', '', regex=True)
                   .replace('', pd.NA)
                   .apply(lambda x: str(x).zfill(13)[:13] if pd.notna(x) else pd.NA))
    
    df['æ—§å•†å“å'] = df['æ—§å•†å“å'].replace('', 'è©²å½“æ–‡å­—åˆ—ãªã—')
    df['æ–°å•†å“å'] = df['æ–°å•†å“å'].replace('', 'è©²å½“æ–‡å­—åˆ—ãªã—')
    
    return df[df['æ—§JANã‚³ãƒ¼ãƒ‰'] != df['æ–°JANã‚³ãƒ¼ãƒ‰']].drop_duplicates()


def run_kao_planet_process(kao_files, planet_paths_dict) -> pd.DataFrame:
    """èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†å®Ÿè¡Œï¼ˆçµ±åˆç”¨ï¼‰"""
    print("\nğŸ­ èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†é–‹å§‹...")
    
    combined_df = pd.DataFrame()
    
    # èŠ±ç‹å‡¦ç†
    if kao_files:
        kao_df = pd.concat([load_kao(p) for p in kao_files], ignore_index=True)
        kao_df = kao_df.rename(columns={'å‚™è€ƒ': 'æ–°JANå‚™è€ƒ'})
        combined_df = pd.concat([combined_df, kao_df], ignore_index=True)
    
    # ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†
    if planet_paths_dict:
        planet_result = []
        for season, paths in planet_paths_dict.items():
            try:
                new_df = load_with_repair(paths['new'], dtype={'ï¼ªï¼¡ï¼®ã‚³ãƒ¼ãƒ‰': str, 'æ—§ï¼ªï¼¡ï¼®ã‚³ãƒ¼ãƒ‰': str})
                new_df['å‚™è€ƒ'] = paths['new'].name
                
                disc_df = load_with_repair(paths['disc'], dtype={'JANã‚³ãƒ¼ãƒ‰': str, 'æ–°JANã‚³ãƒ¼ãƒ‰': str})
                disc_df['å‚™è€ƒ'] = paths['disc'].name
                
                new_df = exclude_kao(new_df, 'ãƒ¡ãƒ¼ã‚«ãƒ¼ã‚³ãƒ¼ãƒ‰')
                disc_df = exclude_kao(disc_df, 'ãƒ¡ãƒ¼ã‚«ãƒ¼')
                
                new_clean = clean_planet(new_df, 'new')
                disc_clean = clean_planet(disc_df, 'discontinue')
                
                disc_not_in_new = disc_clean[~disc_clean['æ–°JAN'].isin(new_clean['æ–°JAN'])].copy()
                final_disc_additions = disc_not_in_new[~disc_not_in_new['æ—§JAN'].isin(new_clean['æ—§JAN'])].copy()
                
                pure_new_items = extract_unmatched(new_clean, disc_clean)
                
                combined_planet = pd.concat([pure_new_items, final_disc_additions], ignore_index=True)
                combined_planet_with_notes = pd.merge(
                    combined_planet,
                    new_df[['JANã‚³ãƒ¼ãƒ‰', 'å‚™è€ƒ']],
                    left_on='æ–°JAN',
                    right_on='JANã‚³ãƒ¼ãƒ‰',
                    how='left'
                ).drop(columns='JANã‚³ãƒ¼ãƒ‰').rename(columns={'å‚™è€ƒ': 'æ–°JANå‚™è€ƒ'})
                
                planet_result.append(combined_planet_with_notes)
            except Exception as e:
                print(f"âŒ {season}å‡¦ç†å¤±æ•—: {e}")
                continue
        
        if planet_result:
            planet_df = pd.concat(planet_result, ignore_index=True)
            combined_df = pd.concat([combined_df, planet_df], ignore_index=True)
    
    if combined_df.empty:
        return pd.DataFrame()
    
    final_df = finalize_kao_planet(combined_df)
    print(f"âœ… èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†å®Œäº†: {len(final_df)}ä»¶")
    
    return final_df


# ========================================================================
# çµ±åˆå‡¦ç†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
# ========================================================================

def normalize_matching_columns(df: pd.DataFrame) -> pd.DataFrame:
    """ãƒãƒƒãƒãƒ³ã‚°çµæœã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
    df_normalized = df.rename(columns={
        'JANã‚³ãƒ¼ãƒ‰_æ—§': 'æ—§JANã‚³ãƒ¼ãƒ‰',
        'JANã‚³ãƒ¼ãƒ‰_æ–°': 'æ–°JANã‚³ãƒ¼ãƒ‰',
        'å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ—§': 'æ—§å•†å“å',
        'å•†å“åç§°ï¼ˆã‚«ãƒŠï¼‰_æ–°': 'æ–°å•†å“å',
        'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°_æ–°': 'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°',
    }).copy()
    
    df_normalized['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'] = 'ãƒãƒƒãƒãƒ³ã‚°'
    df_normalized['å‡¦ç†æ—¥'] = datetime.now().strftime('%Y-%m-%d')
    df_normalized['å‚™è€ƒ'] = df_normalized.get('ç…§åˆçµæœ', '')
    
    return df_normalized[['æ—§JANã‚³ãƒ¼ãƒ‰', 'æ—§å•†å“å', 'æ–°JANã‚³ãƒ¼ãƒ‰', 'æ–°å•†å“å',
                          'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°', 'å‚™è€ƒ', 'å‡¦ç†æ—¥', 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹']]


def normalize_kao_planet_columns(df: pd.DataFrame) -> pd.DataFrame:
    """èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆçµæœã‚’çµ±ä¸€ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«å¤‰æ›"""
    df_normalized = df.copy()
    
    if 'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°' not in df_normalized.columns:
        df_normalized['ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°'] = ''
    
    if 'å‚™è€ƒ' not in df_normalized.columns:
        df_normalized['å‚™è€ƒ'] = df_normalized.get('æ–°JANå‚™è€ƒ', '')
    
    df_normalized['ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹'] = 'èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆ'
    df_normalized['å‡¦ç†æ—¥'] = datetime.now().strftime('%Y-%m-%d')
    
    return df_normalized[['æ—§JANã‚³ãƒ¼ãƒ‰', 'æ—§å•†å“å', 'æ–°JANã‚³ãƒ¼ãƒ‰', 'æ–°å•†å“å',
                          'ãƒ¡ãƒ¼ã‚«ãƒ¼åç§°', 'å‚™è€ƒ', 'å‡¦ç†æ—¥', 'ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹']]


def merge_and_deduplicate(existing_df, df_kao_planet, df_matching) -> pd.DataFrame:
    """ãƒ‡ãƒ¼ã‚¿çµ±åˆã¨é‡è¤‡å‰Šé™¤"""
    print("\nğŸ“¦ ãƒ‡ãƒ¼ã‚¿çµ±åˆé–‹å§‹...")
    
    print(f"  æ—¢å­˜: {len(existing_df)}ä»¶")
    print(f"  èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆ: {len(df_kao_planet)}ä»¶")
    print(f"  ãƒãƒƒãƒãƒ³ã‚°: {len(df_matching)}ä»¶")
    
    all_data = pd.concat([existing_df, df_kao_planet, df_matching], ignore_index=True)
    print(f"  çµ±åˆå¾Œ: {len(all_data)}ä»¶")
    
    for col in ['æ—§JANã‚³ãƒ¼ãƒ‰', 'æ–°JANã‚³ãƒ¼ãƒ‰']:
        all_data[col] = (all_data[col].astype(str)
                         .str.replace(r'\D+', '', regex=True)
                         .str.zfill(13).str[:13])
    
    before_dedup = len(all_data)
    all_data = all_data.drop_duplicates(subset=['æ–°JANã‚³ãƒ¼ãƒ‰'], keep='first')
    after_dedup = len(all_data)
    
    print(f"  é‡è¤‡å‰Šé™¤: {before_dedup - after_dedup}ä»¶")
    
    all_data = all_data[all_data['æ—§JANã‚³ãƒ¼ãƒ‰'] != all_data['æ–°JANã‚³ãƒ¼ãƒ‰']]
    print(f"  æœ€çµ‚ä»¶æ•°: {len(all_data)}ä»¶")
    
    return all_data


def load_existing_data(ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«: Path) -> pd.DataFrame:
    """æ—¢å­˜ç´¯ç©ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿"""
    if ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«.exists():
        try:
            df = pd.read_csv(ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«, dtype={'æ—§JANã‚³ãƒ¼ãƒ‰': str, 'æ–°JANã‚³ãƒ¼ãƒ‰': str}, encoding='utf-8')
            print(f"ğŸ“‚ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(df)}ä»¶")
            return df
        except Exception as e:
            print(f"âš ï¸ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return pd.DataFrame()
    else:
        print("ğŸ“‚ æ–°è¦ä½œæˆãƒ¢ãƒ¼ãƒ‰")
        return pd.DataFrame()


# ========================================================================
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ========================================================================

def main():
    """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    print("=" * 60)
    print("çµ±åˆãƒã‚¹ã‚¿ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ  - é€±æ¬¡å®Ÿè¡Œç‰ˆ")
    print("=" * 60)
    
    # å‡ºåŠ›å…ˆé¸æŠ
    root = tk.Tk()
    root.withdraw()
    output_dir = filedialog.askdirectory(title="ç´¯ç©ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆã‚’é¸æŠ")
    root.destroy()
    
    if not output_dir:
        messagebox.showwarning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", "ãƒ•ã‚©ãƒ«ãƒ€æœªé¸æŠ")
        return
    
    output_dir = Path(output_dir)
    ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«_csv = output_dir / "ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ.csv"
    ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«_excel = output_dir / "ç´¯ç©_å·®ã—æ›¿ãˆãƒªã‚¹ãƒˆ.xlsx"
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
    existing_df = load_existing_data(ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«_csv)
    
    # èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†
    kao_files = []
    planet_paths = {}
    
    if messagebox.askyesno("èŠ±ç‹å‡¦ç†", "èŠ±ç‹ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã‹ï¼Ÿ"):
        root = tk.Tk()
        root.withdraw()
        kao_paths = filedialog.askopenfilenames(title="èŠ±ç‹ãƒ•ã‚¡ã‚¤ãƒ«é¸æŠï¼ˆè¤‡æ•°å¯ï¼‰",
                                                filetypes=[("Excel", "*.xlsm *.xlsx")])
        root.destroy()
        kao_files = [Path(p) for p in kao_paths]
    
    if messagebox.askyesno("ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†", "ãƒ—ãƒ©ãƒãƒƒãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†ã—ã¾ã™ã‹ï¼Ÿ"):
        root = tk.Tk()
        root.withdraw()
        
        messagebox.showinfo("é¸æŠ", "ä¸ŠæœŸ æ–°è¦å“ãƒªã‚¹ãƒˆã‚’é¸æŠ")
        new_upper = filedialog.askopenfilename(title="ä¸ŠæœŸ æ–°è¦å“", filetypes=[("Excel", "*.xlsx")])
        
        messagebox.showinfo("é¸æŠ", "ä¸ŠæœŸ å»ƒç•ªå“ãƒªã‚¹ãƒˆã‚’é¸æŠ")
        disc_upper = filedialog.askopenfilename(title="ä¸ŠæœŸ å»ƒç•ªå“", filetypes=[("Excel", "*.xlsx")])
        
        messagebox.showinfo("é¸æŠ", "ä¸‹æœŸ æ–°è¦å“ãƒªã‚¹ãƒˆã‚’é¸æŠ")
        new_lower = filedialog.askopenfilename(title="ä¸‹æœŸ æ–°è¦å“", filetypes=[("Excel", "*.xlsx")])
        
        messagebox.showinfo("é¸æŠ", "ä¸‹æœŸ å»ƒç•ªå“ãƒªã‚¹ãƒˆã‚’é¸æŠ")
        disc_lower = filedialog.askopenfilename(title="ä¸‹æœŸ å»ƒç•ªå“", filetypes=[("Excel", "*.xlsx")])
        
        root.destroy()
        
        if new_upper and disc_upper:
            planet_paths["ä¸ŠæœŸ"] = {"new": Path(new_upper), "disc": Path(disc_upper)}
        if new_lower and disc_lower:
            planet_paths["ä¸‹æœŸ"] = {"new": Path(new_lower), "disc": Path(disc_lower)}
    
    # èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†å®Ÿè¡Œ
    df_kao_planet = pd.DataFrame()
    if kao_files or planet_paths:
        try:
            df_kao_planet_raw = run_kao_planet_process(kao_files, planet_paths)
            if not df_kao_planet_raw.empty:
                df_kao_planet = normalize_kao_planet_columns(df_kao_planet_raw)
        except Exception as e:
            print(f"âŒ èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆå‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†
    df_matching = pd.DataFrame()
    if messagebox.askyesno("ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†", "ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚’å®Ÿè¡Œã—ã¾ã™ã‹ï¼Ÿ"):
        root = tk.Tk()
        root.withdraw()
        
        # --- ğŸ’¥ ã“ã“ã‹ã‚‰ä¿®æ­£ç®‡æ‰€ ğŸ’¥ ---
        # è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ç¢ºå®Ÿã«é¸æŠã§ãã‚‹ã‚ˆã†ã« filetypes ã®ãƒªã‚¹ãƒˆã‚’ä¿®æ­£ã—ã¦ã„ã‚‹ã‚ˆ
        # ãªã‚“ã§ãã†ã—ã¦ã‚‹ã‹: "*.xlsx *.csv*.tsv" ã®ã‚ˆã†ãªã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã ã¨OSã‚„Pythonã®ãƒãƒ¼ã‚¸ãƒ§ãƒ³ã«ã‚ˆã£ã¦
        # èªè­˜ã•ã‚Œãªã„ã“ã¨ãŒã‚ã‚‹ã‹ã‚‰ã€æ‹¡å¼µå­ã”ã¨ã«å€‹åˆ¥ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¨ã—ã¦æŒ‡å®šã™ã‚‹ã®ãŒç¢ºå®Ÿãªã‚“ã‚„
        all_filetypes = [
            ("Excelãƒ•ã‚¡ã‚¤ãƒ«", "*.xlsx *.xls *.xlsm"), # Excelç³»ã®æ‹¡å¼µå­ã‚’ã¾ã¨ã‚ã¦ã‚‹ã‚ˆ
            ("CSVãƒ•ã‚¡ã‚¤ãƒ«", "*.csv"), # CSVãƒ•ã‚¡ã‚¤ãƒ«ã ã‚ˆ
            ("TSVãƒ•ã‚¡ã‚¤ãƒ«", "*.tsv"), # TSVãƒ•ã‚¡ã‚¤ãƒ«ï¼ˆã‚¿ãƒ–åŒºåˆ‡ã‚Šï¼‰ã ã‚ˆã€‚ã“ã‚Œã§é¸æŠãƒªã‚¹ãƒˆã«å‡ºã¦ãã‚‹ã‚ˆã†ã«ãªã‚‹ã‚ˆ
            ("å…¨ãƒ•ã‚¡ã‚¤ãƒ«", "*.*"), # å¿µã®ãŸã‚ã™ã¹ã¦ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¡¨ç¤ºã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã‚‹ã‚ˆ
        ]
        
        messagebox.showinfo("é¸æŠ", "æ—§ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ")
        old_path = filedialog.askopenfilename(title="æ—§ãƒã‚¹ã‚¿", filetypes=all_filetypes) # ä¿®æ­£ã—ãŸ filetypes ã‚’æ¸¡ã—ã¦ã‚‹ã‚ˆ
        
        if not old_path:
            messagebox.showwarning("ã‚­ãƒ£ãƒ³ã‚»ãƒ«", "æ—§ãƒã‚¹ã‚¿æœªé¸æŠ")
            root.destroy()
        else:
            messagebox.showinfo("é¸æŠ", "æ–°ãƒã‚¹ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ")
            new_path = filedialog.askopenfilename(title="æ–°ãƒã‚¹ã‚¿", filetypes=all_filetypes) # ä¿®æ­£ã—ãŸ filetypes ã‚’æ¸¡ã—ã¦ã‚‹ã‚ˆ
            
            root.destroy()
            
            if new_path:
                try:
                    df_matching_raw = run_matching_process(old_path, new_path)
                    if not df_matching_raw.empty:
                        df_matching = normalize_matching_columns(df_matching_raw)
                except Exception as e:
                    print(f"âŒ ãƒãƒƒãƒãƒ³ã‚°å‡¦ç†ã‚¨ãƒ©ãƒ¼: {e}")
    
    # ãƒ‡ãƒ¼ã‚¿çµ±åˆ
    if df_kao_planet.empty and df_matching.empty:
        messagebox.showwarning("ãƒ‡ãƒ¼ã‚¿ãªã—", "å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãªã—")
        return
    
    final_df = merge_and_deduplicate(existing_df, df_kao_planet, df_matching)
    
    # ä¿å­˜
    print("\nğŸ’¾ ä¿å­˜ä¸­...")
    final_df.to_csv(ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«_csv, index=False, encoding='utf-8')
    final_df.to_excel(ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«_excel, index=False, engine='openpyxl')
    
    # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    summary = f"""
ğŸ‰ å‡¦ç†å®Œäº†ï¼

ã€ç´¯ç©ãƒ‡ãƒ¼ã‚¿ã€‘
ç·ä»¶æ•°: {len(final_df)}ä»¶

ã€ä»Šå›è¿½åŠ ã€‘
èŠ±ç‹ãƒ»ãƒ—ãƒ©ãƒãƒƒãƒˆ: {len(df_kao_planet)}ä»¶
ãƒãƒƒãƒãƒ³ã‚°: {len(df_matching)}ä»¶

ã€ä¿å­˜å…ˆã€‘
{ç´¯ç©ãƒ•ã‚¡ã‚¤ãƒ«_csv}
"""
    
    print(summary)
    messagebox.showinfo("å®Œäº†", summary)


# ========================================================================
# å®Ÿè¡Œ
# ========================================================================

if __name__ == "__main__":
    main()